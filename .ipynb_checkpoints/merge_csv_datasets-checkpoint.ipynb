{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\kukai\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries in this cell\n",
    "import pandas as pd #pandas is a library for data wrangling/handling\n",
    "import numpy as np #same case for numpy\n",
    "\n",
    "# Libraries for helping us with strings\n",
    "import string\n",
    "# Regular Expression Library\n",
    "import re\n",
    "\n",
    "# Seaborn / matplotlib for visualization \n",
    "import seaborn as sns\n",
    "# This command tells python to use seaborn for its styling.\n",
    "sns.set()\n",
    "\n",
    "\n",
    "# Matplotlib is also a very useful, basic visualization/plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "# Very important, this will make your charts appear in your notebook instead of in a new window.\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Provides z-score helper function,\n",
    "# z-score uses standard deviation to remove outliers\n",
    "# (industry standard is if a data point is 3 std devs away from mean,\n",
    "# it's considered to be an outlier)\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# Ignore this, this is just for displaying images.\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# Importing sklearn library\n",
    "import sklearn\n",
    "\n",
    "# Import the trees from sklearn\n",
    "from sklearn import tree\n",
    "\n",
    "# Metrics help us score our model, using metrics to evaluate our model\n",
    "from sklearn import metrics\n",
    "\n",
    "# Import our Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import our Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Import our text vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# This is our Logit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Importing our linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Helper fuctions to evaluate our model from sklearn, including f1_score.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "# Some more helpful ML function\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Helper function to split our data for testing and training purposes\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Helper function for hyper-parameter turning.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import MultinomaialNB classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Import our Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Library for visualizing our tree\n",
    "# If you get an error, \n",
    "# run 'conda install python-graphviz' in your terminal (without the quotes).\n",
    "import graphviz \n",
    "\n",
    "\n",
    "# NLTK is our Natural-Language-Took-Kit\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# You may need to download these from nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns in dataframe: (37249, 2) \n",
      "\n",
      "                                       clean_comment  category\n",
      "0   family mormon have never tried explain them t...         1\n",
      "1  buddhism has very much lot compatible with chr...         1\n",
      "2  seriously don say thing first all they won get...        -1\n",
      "3  what you have learned yours and only yours wha...         0\n",
      "4  for your own benefit you may want read living ...         1\n",
      "******************** \n",
      "Is null:\n",
      "clean_comment    100\n",
      "category           0\n",
      "dtype: int64 \n",
      "\n",
      "% of null and dup in data\n",
      "clean_comment    0.27\n",
      "category         0.00\n",
      "dtype: float64\n",
      "Number of dupes are 350 \n",
      "\n",
      "total number of dupes:  0\n",
      "\n",
      "Number of rows after cleaning data:  36799\n",
      "[ 1 -1  0]\n"
     ]
    }
   ],
   "source": [
    "#First dataframe\n",
    "df_reddit_data = pd.read_csv('data/Reddit_Data.csv')\n",
    "print(\"Number of rows and columns in dataframe: \" + str(df_reddit_data.shape), '\\n')\n",
    "print(df_reddit_data.head())\n",
    "\n",
    "print(\"*\"*20, \"\\nIs null:\")\n",
    "print(df_reddit_data.isnull().sum(), '\\n')\n",
    "print(\"% of null and dup in data\")\n",
    "print(((df_reddit_data.isnull().sum() / len(df_reddit_data)) *100).round(2))\n",
    "# Dropping nulls\n",
    "df_reddit_data.dropna(inplace=True)\n",
    "\n",
    "# Checking for duplicates\n",
    "print(\"Number of dupes are\", df_reddit_data.duplicated().sum(), \"\\n\")\n",
    "\n",
    "#Dropping dupes\n",
    "df_reddit_data.drop_duplicates(inplace=True)\n",
    "\n",
    "#checking dupes\n",
    "print(\"total number of dupes: \",df_reddit_data.duplicated().sum())\n",
    "\n",
    "# total number of rows:\n",
    "print('\\nNumber of rows after cleaning data: ', df_reddit_data.shape[0])\n",
    "\n",
    "print(df_reddit_data.category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['clean_comment', 'category'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for the type of columns, to see if some are faulty\n",
    "# including duplicated columns or faulty ones that don't have an name for example\n",
    "df_reddit_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns in dataframe: (162980, 2) \n",
      "\n",
      "                                          clean_text  category\n",
      "0  when modi promised “minimum government maximum...      -1.0\n",
      "1  talk all the nonsense and continue all the dra...       0.0\n",
      "2  what did just say vote for modi  welcome bjp t...       1.0\n",
      "3  asking his supporters prefix chowkidar their n...       1.0\n",
      "4  answer who among these the most powerful world...       1.0\n",
      "******************** \n",
      "Is null:\n",
      "clean_text    4\n",
      "category      7\n",
      "dtype: int64 \n",
      "\n",
      "% of null and dup in data\n",
      "clean_text    0.0\n",
      "category      0.0\n",
      "dtype: float64\n",
      "Number of dupes are 0 \n",
      "\n",
      "total number of dupes:  0\n",
      "\n",
      "Number of rows after cleaning data:  162969\n",
      "[-1.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "#Second dataframe\n",
    "df_twitter_data = pd.read_csv('data/Twitter_Data.csv')\n",
    "print(\"Number of rows and columns in dataframe: \" + str(df_twitter_data.shape), '\\n')\n",
    "print(df_twitter_data.head())\n",
    "\n",
    "print(\"*\"*20, \"\\nIs null:\")\n",
    "print(df_twitter_data.isnull().sum(), '\\n')\n",
    "print(\"% of null and dup in data\")\n",
    "print(((df_twitter_data.isnull().sum() / len(df_twitter_data)) *100).round(2))\n",
    "# Dropping nulls\n",
    "df_twitter_data.dropna(inplace=True)\n",
    "\n",
    "# Checking for duplicates\n",
    "print(\"Number of dupes are\", df_twitter_data.duplicated().sum(), \"\\n\")\n",
    "\n",
    "#Dropping dupes\n",
    "df_twitter_data.drop_duplicates(inplace=True)\n",
    "\n",
    "#checking dupes\n",
    "print(\"total number of dupes: \",df_twitter_data.duplicated().sum())\n",
    "\n",
    "# total number of rows:\n",
    "print('\\nNumber of rows after cleaning data: ', df_twitter_data.shape[0])\n",
    "\n",
    "print(df_twitter_data.category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean_comment  category\n",
      "0     americunt        -1\n",
      "1      as_hell         -1\n",
      "2          ass         -1\n",
      "3       asshole        -1\n",
      "4       bastard        -1\n",
      "clean_comment    0\n",
      "category         0\n",
      "dtype: int64 \n",
      "\n",
      "clean_comment    0.0\n",
      "category         0.0\n",
      "dtype: float64\n",
      "Number of dupes are 41\n",
      "0\n",
      "Number of nulls: clean_comment    0\n",
      "category         0\n",
      "dtype: int64\n",
      "\n",
      "Number of dupes are 0\n",
      "\n",
      "Number of rows after cleaning data:  1676\n",
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "#Fourth dataframe\n",
    "# change underscores to hypens !!important\n",
    "df_more_bad_words = pd.read_csv('data/more_bad_words.csv', names=[\"clean_comment\"])\n",
    "df_bad_words = pd.read_csv(\"data/bad_words.csv\", names=[\"clean_comment\"])\n",
    "\n",
    "df_profanity = pd.concat([df_more_bad_words, df_bad_words], ignore_index=True)\n",
    "df_profanity[\"category\"] = -1\n",
    "\n",
    "print(df_profanity.head())\n",
    "\n",
    "# Checking for nulls\n",
    "print(df_profanity.isnull().sum(), '\\n')\n",
    "print(((df_profanity.isnull().sum() / len(df_profanity)) *100).round(2))\n",
    "df_profanity.dropna(inplace=True)\n",
    "\n",
    "# Checking for duplicates\n",
    "print(\"Number of dupes are \" + str(df_profanity.duplicated().sum()))\n",
    "\n",
    "#Dropping dupes\n",
    "\n",
    "df_profanity.drop_duplicates(inplace=True)\n",
    "print(df_profanity.duplicated().sum())\n",
    "# Sanity Checking\n",
    "print('Number of nulls: ' + str(df_profanity.isnull().sum()))\n",
    "print(\"\\nNumber of dupes are \" + str(df_profanity.duplicated().sum()))\n",
    "# print(str(df_reddit_data.duplicated()[condition]))\n",
    "print('\\nNumber of rows after cleaning data: ', df_profanity.shape[0])\n",
    "print(df_profanity.category.unique())\n",
    "\n",
    "#Saving the profanity separately as well, just in case\n",
    "filename = 'pkl_files/profanity.pkl'\n",
    "pickle.dump(df_profanity, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human' 'bot']\n",
      "[1 -1]\n"
     ]
    }
   ],
   "source": [
    "# I ran this cell to merge the deepfake dataset with the reddit and twitter datasets\n",
    "\n",
    "# merging the deep fake datasets\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_valid = pd.read_csv(\"data/validation.csv\")\n",
    "\n",
    "df_test = df_test.drop(columns = ['screen_name', 'class_type'])\n",
    "df_train = df_train.drop(columns = ['screen_name', 'class_type'])\n",
    "df_valid = df_valid.drop(columns = ['screen_name', 'class_type'])\n",
    "\n",
    "# have to run this cell again once i change the values in the second column\n",
    "df_list = [df_test, df_train, df_valid]\n",
    "# pd.concat() has a parameter (index_ignore) that will rid us of the problem a useless index\n",
    "df_deepfake = pd.concat(df_list, ignore_index = True)\n",
    "\n",
    "df_deepfake = df_deepfake.rename(columns = {'text' : 'clean_comment', 'account.type' : 'category'})\n",
    "\n",
    "\n",
    "print(df_deepfake[\"category\"].unique())\n",
    "condition1 = df_deepfake['category'] == 'human'\n",
    "condition2 = df_deepfake['category'] == 'bot'\n",
    "\n",
    "df_deepfake.loc[condition1, 'category'] = 1\n",
    "df_deepfake.loc[condition2, 'category'] = -1\n",
    "print(df_deepfake[\"category\"].unique())\n",
    "\n",
    "\n",
    "#saved this as a separate dataframe here using pickle, just in case\n",
    "filename = 'pkl_files/deepfake.pkl'\n",
    "\n",
    "pickle.dump(df_deepfake, open(filename, \"wb\"))\n",
    "\n",
    "df_dpfk = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My attempt at merging the reddit and twitter datasets together\n",
    "df_twitter_data.columns =[\"clean_comment\", \"category\"]\n",
    "\n",
    "list_of_df = [df_reddit_data, df_twitter_data, df_profanity, df_dpfk]\n",
    "\n",
    "df_posts = pd.concat(list_of_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227016\n",
      "(227016, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       clean_comment category\n",
       "0   family mormon have never tried explain them t...        1\n",
       "1  buddhism has very much lot compatible with chr...        1\n",
       "2  seriously don say thing first all they won get...       -1\n",
       "3  what you have learned yours and only yours wha...        0\n",
       "4  for your own benefit you may want read living ...        1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(len(a) for a in list_of_df))\n",
    "print(str(df_posts.shape))\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227016, 2)\n",
      "Index(['clean_comment', 'category'], dtype='object')\n",
      "[1 -1 0]\n"
     ]
    }
   ],
   "source": [
    "# Just some more sanity checking\n",
    "print(df_posts.shape)\n",
    "print(df_posts.columns)\n",
    "print(df_posts.category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1676, 2)\n",
      "1676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>americunt</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as_hell</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ass</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asshole</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bastard</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clean_comment  category\n",
       "0     americunt        -1\n",
       "1      as_hell         -1\n",
       "2          ass         -1\n",
       "3       asshole        -1\n",
       "4       bastard        -1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(str(df_profanity.shape))\n",
    "print(len(df_profanity[\"clean_comment\"].unique()))\n",
    "df_profanity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of profane words that shouldn't be allowed\n",
    "profanity_list = set(df_profanity[\"clean_comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filer words out using profanity from df_bad_words\n",
    "def remove_profanity(profane_str):\n",
    "    words = word_tokenize(profane_str)\n",
    "    valid_words = []\n",
    "    for word in words:\n",
    "        if word not in profanity_list:\n",
    "            valid_words.append(word)\n",
    "    profane_str = ' '.join(valid_words)\n",
    "    return profane_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seems like good practice to me, creating a single function that will call all\n",
    "# our necessary functions from one place, will be subject to change\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def text_pipeline(input_str):\n",
    "    input_str = remove_profanity(input_str)\n",
    "    return input_str\n",
    "\n",
    "def mk_lower(a):\n",
    "    return a.lower()\n",
    "\n",
    "def remove_stopwords(a):\n",
    "    return \" \".join([word for word in word_tokenize(a) if word not in stopwords])\n",
    "\n",
    "def remove_sp_char(a):\n",
    "    ## \\s for white space, ^ is negation, \\w is words.  so replace all punctutation that follows a word \n",
    "    return a.translate(translator)\n",
    "\n",
    "def remove_sp_char2(a):\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", a)\n",
    "    \n",
    "    \n",
    "def text_pipeline2(a):\n",
    "    a = mk_lower(a)\n",
    "    a = remove_sp_char(a)\n",
    "    a = remove_stopwords(a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_posts[\"clean_comment2\"] = df_posts['clean_comment']\n",
    "#df_posts[\"clean_comment2\"] = df_posts['clean_comment'].apply(text_pipeline2)\n",
    "#df_posts['clean_comment_profane_free'] = df_posts['clean_comment2'].apply(text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            clean_comment category\n",
      "0        family mormon have never tried explain them t...        1\n",
      "1       buddhism has very much lot compatible with chr...        1\n",
      "2       seriously don say thing first all they won get...       -1\n",
      "3       what you have learned yours and only yours wha...        0\n",
      "4       for your own benefit you may want read living ...        1\n",
      "...                                                   ...      ...\n",
      "227011  You're going to be even prouder when we don't ...       -1\n",
      "227012    https://t.co/10XkzXDBCf https://t.co/cIUIYWEB45        1\n",
      "227013  2. “Once you take the place of the people who ...        1\n",
      "227014  black will be like a company with them need so...       -1\n",
      "227015  Guys, I hate Facebook. And this Facebook ad ca...       -1\n",
      "\n",
      "[227016 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# clean_comment is the original value of everything, unfiltered\n",
    "# clean_comment2 is the clean_comment column but filtered through a pipeline of functions that filter the text\n",
    "# clean_comment_profane_free is the clean_comment2 column applied with an additional (profanity) filter\n",
    "print(df_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME YOUR MODEL \n",
    "filename = 'pkl_files/comments.pkl'\n",
    "\n",
    "# EXPORT AND SAVE df\n",
    "pickle.dump(df_posts, open(filename, \"wb\"))\n",
    "\n",
    "## HOW TO LOAD IT FOR FUTURE USE\n",
    "df = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            clean_comment category\n",
      "0        family mormon have never tried explain them t...        1\n",
      "1       buddhism has very much lot compatible with chr...        1\n",
      "2       seriously don say thing first all they won get...       -1\n",
      "3       what you have learned yours and only yours wha...        0\n",
      "4       for your own benefit you may want read living ...        1\n",
      "...                                                   ...      ...\n",
      "227011  You're going to be even prouder when we don't ...       -1\n",
      "227012    https://t.co/10XkzXDBCf https://t.co/cIUIYWEB45        1\n",
      "227013  2. “Once you take the place of the people who ...        1\n",
      "227014  black will be like a company with them need so...       -1\n",
      "227015  Guys, I hate Facebook. And this Facebook ad ca...       -1\n",
      "\n",
      "[227016 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "## run this cell if you want to export the data as a raw .csv file\n",
    "mask1 = df[\"category\"] == -1\n",
    "\n",
    "df.loc[mask1, 'category'] = 0\n",
    "print(df[\"category\"].unique())\n",
    "\n",
    "csv_file_name = \"data/cleaned_agg_data.csv\"\n",
    "\n",
    "#df.to_csv(csv_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
